{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saikat-roy/Vision-Systems-Lab/blob/master/Assignment4/Assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcnBLksitVM2",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 4: CudaVision\n",
        "------\n",
        "### Group Members:\n",
        "__1.__ Saikat Roy\n",
        "\n",
        "__2.__ Albert Gubaidullin\n",
        "\n",
        "## Import Dependencies\n",
        "------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "Rt9R8MJttVM3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import dataloader, random_split\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZIwyhqdtVM6",
        "colab_type": "text"
      },
      "source": [
        "## Softmax Regression MLP object\n",
        "------\n",
        "The `SoftmaxRegressionMLP` class is a basic object to create a MLP with ReLU hidden units and a linear output unit (argmax(x) is the same as argmax(softmax(x)). Options while initializing include those of type of non-linearity and the number of layer and number of hidden units."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "IWstC0kjtVM7",
        "colab_type": "code",
        "outputId": "d03f3bf4-de89-4f00-914d-8f26fb5f06fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "\n",
        "    def __init__(self, n_input, n_output, n_hidden_layers=None, h_units=None, pool='max', \n",
        "                 dropout=0.3):\n",
        "        \"\"\"\n",
        "        Initialization for a simply softmax regression MLP model with ReLU activations in hidden layers\n",
        "        :param n_input (int): Number of input units to network\n",
        "        :param n_hidden_layers (int): Number of hidden layers in network\n",
        "        :param n_output (int): Number of output units of network\n",
        "        :param h_units (int or list): hidden unit count or list of hidden units in each hidden layer of network\n",
        "        \"\"\"\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.n_out = n_output\n",
        "\n",
        "        layers = []  \n",
        "        \n",
        "        assert type(h_units) is list and n_hidden_layers is None\n",
        "        \n",
        "        # Add input layers\n",
        "        if type(h_units) is list:\n",
        "            n_hidden_layers = len(h_units)\n",
        "            layers.extend(self.add_convlayer_and_act(n_input, h_units[0], 'relu'))\n",
        "        else:\n",
        "            layers.extend(self.add_convlayer_and_act(n_input, h_units, 'relu'))\n",
        "        \n",
        "        # Add hidden layers\n",
        "        for i in range(1, n_hidden_layers):\n",
        "            if type(h_units) is list:\n",
        "                layers.extend(self.add_convlayer_and_act(h_units[i - 1], \n",
        "                                            h_units[i], 'relu', pool))\n",
        "                #layers.extend([nn.Dropout(p=dropout)])  # Dropout added here after each hidden layer\n",
        "            else:\n",
        "                layers.extend(self.add_convlayer_and_act(h_units, \n",
        "                                            h_units, 'relu', pool))\n",
        "                #layers.extend([nn.Dropout(p=dropout)])  # Dropout added here\n",
        "       \n",
        "        self.block = nn.Sequential(*layers) \n",
        "        \n",
        "        # Add output layer\n",
        "        if type(h_units) is list:\n",
        "          self.last = h_units[-1]\n",
        "        else:\n",
        "          self.last = h_units\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.out = nn.Linear(self.last, n_output)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Simple forward pass\n",
        "        :param x:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        x = self.block(x)\n",
        "        # Global Average Pooling\n",
        "#         print(\"Size before global average pooling: {}\".format(x.size()))\n",
        "        x = F.avg_pool2d(x, x.size()[2:])\n",
        "#         print(\"Size after global average pooling: {}\".format(x.size()))\n",
        "        \n",
        "        x = x.view(-1,self.last) # Converting into vector of \n",
        "                                 # size (batch_size, n_last_hidden)\n",
        "        x = self.drop(x)\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "    \n",
        "    def add_convlayer_and_act(self, n_inp, n_out, nl_type, pool='max'):\n",
        "        \"\"\"\n",
        "        \n",
        "        \"\"\"\n",
        "        layer_list = []\n",
        "        layer_list.extend([nn.Conv2d(n_inp, n_out, kernel_size=(3,3), padding=1), \n",
        "                               nn.BatchNorm2d(n_out), self.non_lin(nl_type)])\n",
        "#         layer_list.extend([nn.Conv2d(n_out, n_out, kernel_size=(3,3)), \n",
        "#                                nn.BatchNorm2d(n_out), self.non_lin(nl_type)])\n",
        "        \n",
        "        if pool is not None:\n",
        "          layer_list.append(self.add_pool(pool))\n",
        "          \n",
        "        return layer_list\n",
        "    \n",
        "    def non_lin(self, nl_type='sigmoid'):\n",
        "        \"\"\"\n",
        "        Simply plugs in a predefined non-linearity from a dictionary to be used throughout the network\n",
        "        :param nl_type: type based on predefined types. Defaults to sigmoid on wrong type.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        nl = {'sigmoid': nn.Sigmoid(), 'relu': nn.ReLU(), 'softmax': nn.Softmax(self.n_out)}\n",
        "        try:\n",
        "            return nl[nl_type]\n",
        "        except:\n",
        "            print(\"non linearity type not found. Defaulting to sigmoid.\")\n",
        "            return nl['sigmoid']\n",
        "          \n",
        "    def add_pool(self, pool_type):\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        p = {\"max\": nn.MaxPool2d(2), \"mean\": nn.AvgPool2d(2)}\n",
        "        assert pool_type in p.keys() # Only max and mean pool supported\n",
        "        return p[pool_type]\n",
        "\n",
        "\n",
        "m = ConvNet(3,10,None,[4,5,6]).cuda()\n",
        "print(summary(m, input_size=(3, 256, 256), batch_size=1))\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [1, 4, 256, 256]             112\n",
            "       BatchNorm2d-2           [1, 4, 256, 256]               8\n",
            "              ReLU-3           [1, 4, 256, 256]               0\n",
            "         MaxPool2d-4           [1, 4, 128, 128]               0\n",
            "            Conv2d-5           [1, 5, 128, 128]             185\n",
            "       BatchNorm2d-6           [1, 5, 128, 128]              10\n",
            "              ReLU-7           [1, 5, 128, 128]               0\n",
            "         MaxPool2d-8             [1, 5, 64, 64]               0\n",
            "            Conv2d-9             [1, 6, 64, 64]             276\n",
            "      BatchNorm2d-10             [1, 6, 64, 64]              12\n",
            "             ReLU-11             [1, 6, 64, 64]               0\n",
            "        MaxPool2d-12             [1, 6, 32, 32]               0\n",
            "          Dropout-13                     [1, 6]               0\n",
            "           Linear-14                    [1, 10]              70\n",
            "================================================================\n",
            "Total params: 673\n",
            "Trainable params: 673\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.75\n",
            "Forward/backward pass size (MB): 9.14\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 9.89\n",
            "----------------------------------------------------------------\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q57m8s6vtVM9",
        "colab_type": "text"
      },
      "source": [
        "## Method for calculating accuracy\n",
        "------\n",
        "The `acc` method simply calculates the predictive accuracy of a model on the given dataloader. It returns the true labels, predicted labels and the accuracy as a float in the range [0,1]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "PvtJq9gZtVM-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def acc(dataloader):\n",
        "    \"\"\"\n",
        "    Calculate accuracy of predictions from model for dataloader.\n",
        "    :param dataloader: dataloader to evaluate\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    acc = 0.0\n",
        "    true_y = []\n",
        "    pred_y = []\n",
        "    total = 0.0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_id, (x, y) in enumerate(dataloader):\n",
        "            x = x.cuda()\n",
        "            y = y.cuda()\n",
        "            #x = x.view(batch_size, -1)\n",
        "            # print(x[0])\n",
        "            preds = model(x)\n",
        "            preds = torch.argmax(preds, dim=1)\n",
        "            acc += ((preds==y).sum().item())\n",
        "            total+= y.size(0)\n",
        "\n",
        "            true_y.extend(list(preds.view(-1).cpu().numpy()))\n",
        "            pred_y.extend(list(y.view(-1).cpu().numpy()))\n",
        "\n",
        "        acc/=total\n",
        "    return true_y, pred_y, acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmYMDfCRtVNA",
        "colab_type": "text"
      },
      "source": [
        "## Method for training model in PyTorch\n",
        "------\n",
        "The `train` method takes a dataloader object and trains the model on it for the specified amount of iterations and returns the loss per iteration as a list. Also calculates the accuracy on the training validation set per epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "pWlVuyKYtVNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_dataloader, valid_dataloader, iters = 20, suppress_output=False,\n",
        "         model_save_path = \"best.pth\"):\n",
        "    \"\"\"\n",
        "    Trains the model on the given dataloader and returns the loss per epoch\n",
        "    :param dataloader: The autoencoder is trained on the dataloader\n",
        "    :param iters: iterations for training\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    loss_l = []\n",
        "    train_acc_l = []\n",
        "    valid_acc_l = []\n",
        "    best_valid_acc = 0.0\n",
        "    equiv_train_acc = 0.0\n",
        "    for itr in range(iters):\n",
        "        av_itr_loss = 0.0\n",
        "        model.train()\n",
        "        for batch_id, (x, y) in enumerate(train_dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            x = x.cuda()\n",
        "            y = y.cuda()\n",
        "            #x = (x>0.5).float() * 1\n",
        "            #x = x.view(batch_size, -1)\n",
        "            # print(x[0])\n",
        "            preds = model(x)\n",
        "            # print((z==1).sum())\n",
        "            batch_loss = loss(preds, y)\n",
        "            batch_loss.backward()\n",
        "            optimizer.step()\n",
        "            av_itr_loss += (1/y.size(0))*batch_loss.item()\n",
        "        loss_l.append(av_itr_loss)\n",
        "        _, _, train_acc = acc(train_dataloader)\n",
        "        _, _, valid_acc = acc(valid_dataloader)\n",
        "        if not suppress_output:\n",
        "          if itr%1 == 0 or itr==iters-1:\n",
        "            print(\"Epoch {}: Loss={}, Training Accuracy:{}, Validation Accuracy:{}\"\n",
        "                  .format(itr, av_itr_loss, train_acc, valid_acc))\n",
        "        train_acc_l.append(train_acc)\n",
        "        valid_acc_l.append(valid_acc)\n",
        "        if valid_acc>best_valid_acc:\n",
        "          best_valid_acc = valid_acc\n",
        "          equiv_train_acc = train_acc\n",
        "          torch.save(model.state_dict(), model_save_path)\n",
        "    \n",
        "    model.load_state_dict(torch.load(model_save_path))\n",
        "    \n",
        "#   return loss_l, train_acc_l, valid_acc_l\n",
        "    return loss_l, equiv_train_acc, best_valid_acc\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_fxRV6gtVND",
        "colab_type": "text"
      },
      "source": [
        "## Method for plotting confusion matrix\n",
        "------\n",
        "__Please note that this function has been borrowed from the sklearn tutorial regarding the visualization of confusion matrices__. The function has however had minor modification to shorten its outputs and increase the figure size.\n",
        "\n",
        "https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "M2TdrYvGtVND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "\"\"\"\n",
        "THIS CONFUSION MATRIX FUNCTION HAS BEEN BORROWED FROM THE SCIKIT-LEARN TUTORIAL ON GENERATING VISUAL CONFUSION MATRIX\n",
        "PLOTS. REINVENTING THE WHEEL IN THIS CASE SEEMED TO BE EXTREMELY REDUNDANT.\n",
        "\"\"\"\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    # Only use the labels that appear in the data\n",
        "#     print(unique_labels(y_true, y_pred))\n",
        "#     classes = classes[unique_labels(y_true, y_pred)]\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "#   print(cm)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "    \n",
        "    # Resize the subplots\n",
        "    plt.subplots_adjust(left=0.0, right=1.0, bottom=0.0, top=1.0)\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "#     fig.tight_layout()\n",
        "    return ax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlE9H_fXtVNG",
        "colab_type": "text"
      },
      "source": [
        "## Initializing Hyperparameters and Datasets\n",
        "-------\n",
        "The base Datasets, the test Dataloaders and some training hyperparameters are initialized. The images are resized to `64x64` to allow for a suitably deep CNN to be used for classification . Different types of non-linearities are tested later on the best model from here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "E0lc2qo8tVNG",
        "colab_type": "code",
        "outputId": "9f05ab7a-619a-4a1e-f9c3-db41345f5168",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "batch_size = 256\n",
        "n_itr = 25\n",
        "lr = 0.001\n",
        "\n",
        "transform_list = transforms.Compose([transforms.Resize(64),\n",
        "                                     transforms.ToTensor()])\n",
        "trainset = datasets.CIFAR10('./data', train=True, download=True, transform=transform_list)\n",
        "testset = datasets.CIFAR10('./data', train=False, download=True, transform=transform_list)\n",
        "\n",
        "test_dataloader  = torch.utils.data.DataLoader(testset,  batch_size=batch_size, shuffle=False, drop_last=True)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz8A9y6ytVNL",
        "colab_type": "text"
      },
      "source": [
        "## Training the model with 1 convolution  before pooling\n",
        "------\n",
        "The model is trained using models with different number of filters per layer training algorithms. The architecture is kept basic with sequences of `Conv2d-BatchNorm2d-ReLU-Max/AvgPool2d` blocks.  We use both the generic uniform number of convolutions filters per layers as well as the `VGG` way of doubling filters after downsampling.\n",
        "\n",
        "###   Generic Way (All layers with uniform feature maps)\n",
        "\n",
        "Here we train with 64 filters in each layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "RpAS8nKstVNL",
        "colab_type": "code",
        "outputId": "e7a9c974-dedb-4b96-bb91-811fceee6e63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1139
        }
      },
      "source": [
        "# torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "loss_type = \"Cross_Entropy\"\n",
        "model_layers = [5]\n",
        "h_units = 64\n",
        "optim = torch.optim.Adam\n",
        "lr = 0.0005\n",
        "\n",
        "# print(axes)\n",
        "\n",
        "train_split, valid_split = random_split(trainset, [ int(len(trainset)*0.8), \n",
        "                                        int(len(trainset)-(len(trainset)*0.8))])\n",
        "train_dataloader = torch.utils.data.DataLoader(train_split, batch_size=batch_size,\n",
        "                                               shuffle=True, drop_last=True)\n",
        "valid_dataloader = torch.utils.data.DataLoader(valid_split, batch_size=batch_size,\n",
        "                                               shuffle=True, drop_last=True)\n",
        "\n",
        "\n",
        "for i in range(len(model_layers)):\n",
        "    t1 = time.time()\n",
        "    \n",
        "    model_layer = model_layers[i]\n",
        "    #color = colors[i]\n",
        "\n",
        "    print(\"\\nTraining using {} layers:\".format(model_layer))\n",
        "    \n",
        "    h_unit_list = [int(h_units*1**(i+1)) for i in range(model_layer)]\n",
        "    #h_unit_list = [h_units for i in range(1,model_layer+1)]\n",
        "    print(h_unit_list)\n",
        "    model = ConvNet(3, 10, None, h_unit_list).cuda()\n",
        "#   print(model)\n",
        "    summary(model, (3,64,64))\n",
        "    model.train()\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    optimizer = optim(model.parameters(), lr=lr)\n",
        "    loss_list, train_acc, valid_acc = train(train_dataloader, valid_dataloader, \n",
        "                                            iters=n_itr, model_save_path=\"model1.pth\")\n",
        "    _,_, test_acc = acc(test_dataloader)\n",
        "    t2 = time.time()\n",
        "    print(\"Time to converge: {} sec\".format(t2-t1))\n",
        "    print(\"Best train accuracy={}, valid accuracy={} (based on the later)\".format(train_acc, valid_acc))\n",
        "    print(\"Test accuracy on best model={}\".format(test_acc))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training using 5 layers:\n",
            "[64, 64, 64, 64, 64]\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 64, 64]           1,792\n",
            "       BatchNorm2d-2           [-1, 64, 64, 64]             128\n",
            "              ReLU-3           [-1, 64, 64, 64]               0\n",
            "         MaxPool2d-4           [-1, 64, 32, 32]               0\n",
            "            Conv2d-5           [-1, 64, 32, 32]          36,928\n",
            "       BatchNorm2d-6           [-1, 64, 32, 32]             128\n",
            "              ReLU-7           [-1, 64, 32, 32]               0\n",
            "         MaxPool2d-8           [-1, 64, 16, 16]               0\n",
            "            Conv2d-9           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-10           [-1, 64, 16, 16]             128\n",
            "             ReLU-11           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-12             [-1, 64, 8, 8]               0\n",
            "           Conv2d-13             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-14             [-1, 64, 8, 8]             128\n",
            "             ReLU-15             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-16             [-1, 64, 4, 4]               0\n",
            "           Conv2d-17             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-18             [-1, 64, 4, 4]             128\n",
            "             ReLU-19             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-20             [-1, 64, 2, 2]               0\n",
            "          Dropout-21                   [-1, 64]               0\n",
            "           Linear-22                   [-1, 10]             650\n",
            "================================================================\n",
            "Total params: 150,794\n",
            "Trainable params: 150,794\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.05\n",
            "Forward/backward pass size (MB): 8.66\n",
            "Params size (MB): 0.58\n",
            "Estimated Total Size (MB): 9.28\n",
            "----------------------------------------------------------------\n",
            "Epoch 0: Loss=1.0357439052313566, Training Accuracy:0.5026542467948718, Validation Accuracy:0.49529246794871795\n",
            "Epoch 1: Loss=0.7701011877506971, Training Accuracy:0.6246494391025641, Validation Accuracy:0.6067708333333334\n",
            "Epoch 2: Loss=0.6419496613088995, Training Accuracy:0.6598557692307693, Validation Accuracy:0.6438301282051282\n",
            "Epoch 3: Loss=0.5655350084416568, Training Accuracy:0.6361678685897436, Validation Accuracy:0.6138822115384616\n",
            "Epoch 4: Loss=0.511184910312295, Training Accuracy:0.7425130208333334, Validation Accuracy:0.7043269230769231\n",
            "Epoch 5: Loss=0.46956296171993017, Training Accuracy:0.7200520833333334, Validation Accuracy:0.6799879807692307\n",
            "Epoch 6: Loss=0.4328058904502541, Training Accuracy:0.7828275240384616, Validation Accuracy:0.7309695512820513\n",
            "Epoch 7: Loss=0.40420334599912167, Training Accuracy:0.7481219951923077, Validation Accuracy:0.6974158653846154\n",
            "Epoch 8: Loss=0.3756694858893752, Training Accuracy:0.7879356971153846, Validation Accuracy:0.7261618589743589\n",
            "Epoch 9: Loss=0.3554005565820262, Training Accuracy:0.7641476362179487, Validation Accuracy:0.7018229166666666\n",
            "Epoch 10: Loss=0.33315450011286885, Training Accuracy:0.8105969551282052, Validation Accuracy:0.7369791666666666\n",
            "Epoch 11: Loss=0.3133329425472766, Training Accuracy:0.8324569310897436, Validation Accuracy:0.7439903846153846\n",
            "Epoch 12: Loss=0.2991841008188203, Training Accuracy:0.8477063301282052, Validation Accuracy:0.7576121794871795\n",
            "Epoch 13: Loss=0.2790092045906931, Training Accuracy:0.8555939503205128, Validation Accuracy:0.7587139423076923\n",
            "Epoch 14: Loss=0.2608317844569683, Training Accuracy:0.7727113381410257, Validation Accuracy:0.6938100961538461\n",
            "Epoch 15: Loss=0.24937563808634877, Training Accuracy:0.8687399839743589, Validation Accuracy:0.7676282051282052\n",
            "Epoch 16: Loss=0.23373869224451482, Training Accuracy:0.8608523637820513, Validation Accuracy:0.7559094551282052\n",
            "Epoch 17: Loss=0.22413187561323866, Training Accuracy:0.9081280048076923, Validation Accuracy:0.7817508012820513\n",
            "Epoch 18: Loss=0.20812897046562284, Training Accuracy:0.8257461939102564, Validation Accuracy:0.7200520833333334\n",
            "Epoch 19: Loss=0.19706189836142585, Training Accuracy:0.8895733173076923, Validation Accuracy:0.7616185897435898\n",
            "Epoch 20: Loss=0.18915265757823363, Training Accuracy:0.8553435496794872, Validation Accuracy:0.7369791666666666\n",
            "Epoch 21: Loss=0.1764783118851483, Training Accuracy:0.8953575721153846, Validation Accuracy:0.75390625\n",
            "Epoch 22: Loss=0.17033467843430117, Training Accuracy:0.9228766025641025, Validation Accuracy:0.7718349358974359\n",
            "Epoch 23: Loss=0.15377824706956744, Training Accuracy:0.9225510817307693, Validation Accuracy:0.7754407051282052\n",
            "Epoch 24: Loss=0.1507702234084718, Training Accuracy:0.9310396634615384, Validation Accuracy:0.7775440705128205\n",
            "Time to converge: 883.8210361003876 sec\n",
            "Best train accuracy=0.9081280048076923, valid accuracy=0.7817508012820513 (based on the later)\n",
            "Test accuracy on best model=0.7818509615384616\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bjb-HrtsOJxr",
        "colab_type": "text"
      },
      "source": [
        "### Doubling filters after convolution \n",
        "\n",
        "Here we train with `16-32-64-128` filters in each layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnf5_1O1OVF0",
        "colab_type": "code",
        "outputId": "34068192-3b0f-48c0-857a-9b20f0890af5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1139
        }
      },
      "source": [
        "# torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "loss_type = \"Cross_Entropy\"\n",
        "model_layers = [5]\n",
        "h_units = 16\n",
        "optim = torch.optim.Adam\n",
        "lr = 0.0005\n",
        "\n",
        "# print(axes)\n",
        "\n",
        "train_split, valid_split = random_split(trainset, [ int(len(trainset)*0.8), \n",
        "                                        int(len(trainset)-(len(trainset)*0.8))])\n",
        "train_dataloader = torch.utils.data.DataLoader(train_split, batch_size=batch_size,\n",
        "                                               shuffle=True, drop_last=True)\n",
        "valid_dataloader = torch.utils.data.DataLoader(valid_split, batch_size=batch_size,\n",
        "                                               shuffle=True, drop_last=True)\n",
        "\n",
        "\n",
        "for i in range(len(model_layers)):\n",
        "    t1 = time.time()\n",
        "    \n",
        "    model_layer = model_layers[i]\n",
        "    #color = colors[i]\n",
        "\n",
        "    print(\"\\nTraining using {} layers:\".format(model_layer))\n",
        "    \n",
        "    h_unit_list = [int(h_units*2**(i)) for i in range(model_layer)]\n",
        "    #h_unit_list = [h_units for i in range(1,model_layer+1)]\n",
        "    print(h_unit_list)\n",
        "    model = ConvNet(3, 10, None, h_unit_list).cuda()\n",
        "#   print(model)\n",
        "    summary(model, (3,64,64))\n",
        "    model.train()\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    optimizer = optim(model.parameters(), lr=lr)\n",
        "    loss_list, train_acc, valid_acc = train(train_dataloader, valid_dataloader, \n",
        "                                            iters=n_itr, model_save_path=\"model2.pth\")\n",
        "    _, _, test_acc = acc(test_dataloader)\n",
        "    t2 = time.time()\n",
        "    print(\"Time to converge: {} sec\".format(t2-t1))\n",
        "    print(\"Best train accuracy={}, valid accuracy={} (based on the later)\".format(train_acc, valid_acc))\n",
        "    print(\"Test accuracy on best model={}\".format(test_acc))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training using 5 layers:\n",
            "[16, 32, 64, 128, 256]\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 64, 64]             448\n",
            "       BatchNorm2d-2           [-1, 16, 64, 64]              32\n",
            "              ReLU-3           [-1, 16, 64, 64]               0\n",
            "         MaxPool2d-4           [-1, 16, 32, 32]               0\n",
            "            Conv2d-5           [-1, 32, 32, 32]           4,640\n",
            "       BatchNorm2d-6           [-1, 32, 32, 32]              64\n",
            "              ReLU-7           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-8           [-1, 32, 16, 16]               0\n",
            "            Conv2d-9           [-1, 64, 16, 16]          18,496\n",
            "      BatchNorm2d-10           [-1, 64, 16, 16]             128\n",
            "             ReLU-11           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-12             [-1, 64, 8, 8]               0\n",
            "           Conv2d-13            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-14            [-1, 128, 8, 8]             256\n",
            "             ReLU-15            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-16            [-1, 128, 4, 4]               0\n",
            "           Conv2d-17            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-18            [-1, 256, 4, 4]             512\n",
            "             ReLU-19            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-20            [-1, 256, 2, 2]               0\n",
            "          Dropout-21                  [-1, 256]               0\n",
            "           Linear-22                   [-1, 10]           2,570\n",
            "================================================================\n",
            "Total params: 396,170\n",
            "Trainable params: 396,170\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.05\n",
            "Forward/backward pass size (MB): 3.15\n",
            "Params size (MB): 1.51\n",
            "Estimated Total Size (MB): 4.71\n",
            "----------------------------------------------------------------\n",
            "Epoch 0: Loss=0.9213354187086225, Training Accuracy:0.5420172275641025, Validation Accuracy:0.5276442307692307\n",
            "Epoch 1: Loss=0.6898961863480508, Training Accuracy:0.5946013621794872, Validation Accuracy:0.5689102564102564\n",
            "Epoch 2: Loss=0.587241631001234, Training Accuracy:0.6285306490384616, Validation Accuracy:0.6036658653846154\n",
            "Epoch 3: Loss=0.5146205578930676, Training Accuracy:0.7234825721153846, Validation Accuracy:0.6816907051282052\n",
            "Epoch 4: Loss=0.4614995534066111, Training Accuracy:0.7514773637820513, Validation Accuracy:0.6966145833333334\n",
            "Epoch 5: Loss=0.41528611816465855, Training Accuracy:0.7128405448717948, Validation Accuracy:0.6513421474358975\n",
            "Epoch 6: Loss=0.37465197243727744, Training Accuracy:0.7673527644230769, Validation Accuracy:0.6944110576923077\n",
            "Epoch 7: Loss=0.3406420830870047, Training Accuracy:0.7928685897435898, Validation Accuracy:0.7081330128205128\n",
            "Epoch 8: Loss=0.31054773088544607, Training Accuracy:0.7673026842948718, Validation Accuracy:0.6751802884615384\n",
            "Epoch 9: Loss=0.2776783349690959, Training Accuracy:0.8091446314102564, Validation Accuracy:0.6974158653846154\n",
            "Epoch 10: Loss=0.248663850245066, Training Accuracy:0.8553185096153846, Validation Accuracy:0.7228565705128205\n",
            "Epoch 11: Loss=0.2270875402027741, Training Accuracy:0.8292267628205128, Validation Accuracy:0.7013221153846154\n",
            "Epoch 12: Loss=0.20315797463990748, Training Accuracy:0.8741736778846154, Validation Accuracy:0.7171474358974359\n",
            "Epoch 13: Loss=0.18465387425385416, Training Accuracy:0.8455028044871795, Validation Accuracy:0.6988181089743589\n",
            "Epoch 14: Loss=0.16735255491221324, Training Accuracy:0.8111728766025641, Validation Accuracy:0.6638621794871795\n",
            "Epoch 15: Loss=0.14326881064334884, Training Accuracy:0.9482922676282052, Validation Accuracy:0.7511017628205128\n",
            "Epoch 16: Loss=0.12899834319250658, Training Accuracy:0.8444010416666666, Validation Accuracy:0.6892027243589743\n",
            "Epoch 17: Loss=0.12065907591022551, Training Accuracy:0.9021935096153846, Validation Accuracy:0.7066306089743589\n",
            "Epoch 18: Loss=0.1066994807915762, Training Accuracy:0.9108824118589743, Validation Accuracy:0.7209535256410257\n",
            "Epoch 19: Loss=0.0985291734978091, Training Accuracy:0.9099559294871795, Validation Accuracy:0.7102363782051282\n",
            "Epoch 20: Loss=0.08581524595501833, Training Accuracy:0.8897986778846154, Validation Accuracy:0.6905048076923077\n",
            "Epoch 21: Loss=0.07875980640528724, Training Accuracy:0.8285506810897436, Validation Accuracy:0.6529447115384616\n",
            "Epoch 22: Loss=0.07408368153846823, Training Accuracy:0.9406800881410257, Validation Accuracy:0.7191506410256411\n",
            "Epoch 23: Loss=0.07052921838476323, Training Accuracy:0.9114332932692307, Validation Accuracy:0.70703125\n",
            "Epoch 24: Loss=0.05903752849553712, Training Accuracy:0.9352964743589743, Validation Accuracy:0.7198517628205128\n",
            "Time to converge: 643.9723658561707 sec\n",
            "Best train accuracy=0.9482922676282052, valid accuracy=0.7511017628205128 (based on the later)\n",
            "Test accuracy on best model=0.7496995192307693\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aa2ZTiP35leT",
        "colab_type": "text"
      },
      "source": [
        "## Training the model with 2 convolutions  before pooling\n",
        "------\n",
        "The model is trained using models with different number of filters per layer training algorithms. The architecture is kept basic with sequences of `Conv2d-BatchNorm2d-ReLU-Conv2d-BatchNorm2d-ReLU-Max/AvgPool2d` blocks.  We use both the generic uniform number of convolutions filters per layers as well as the `VGG` way of doubling filters after downsampling.\n",
        "\n",
        "\n",
        "### Redefining ConvNet object\n",
        "-----\n",
        "Only `add_convlayer_and_act` function has been overridden."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy4M_xSgVFmM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvNet2(ConvNet):\n",
        " \n",
        "  def add_convlayer_and_act(self, n_inp, n_out, nl_type, pool='max'):\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    layer_list = []\n",
        "    layer_list.extend([nn.Conv2d(n_inp, n_out, kernel_size=(3,3), padding=1), \n",
        "                           nn.BatchNorm2d(n_out), self.non_lin(nl_type)])\n",
        "    layer_list.extend([nn.Conv2d(n_out, n_out, kernel_size=(3,3), padding=1), \n",
        "                           nn.BatchNorm2d(n_out), self.non_lin(nl_type)])\n",
        "\n",
        "    if pool is not None:\n",
        "      layer_list.append(self.add_pool(pool))\n",
        "\n",
        "    return layer_list "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8EIAceHaIV7",
        "colab_type": "text"
      },
      "source": [
        "### Generic Way (All layers with uniform feature maps)\n",
        "\n",
        "Here we train with 64 filters in each layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr_Rro5u6due",
        "colab_type": "code",
        "outputId": "202def89-493d-4be6-a5dc-d5bf6a405258",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1394
        }
      },
      "source": [
        "# torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "loss_type = \"Cross_Entropy\"\n",
        "model_layers = [5]\n",
        "h_units = 64\n",
        "optim = torch.optim.Adam\n",
        "lr = 0.0005\n",
        "\n",
        "# print(axes)\n",
        "\n",
        "train_split, valid_split = random_split(trainset, [ int(len(trainset)*0.8), \n",
        "                                        int(len(trainset)-(len(trainset)*0.8))])\n",
        "train_dataloader = torch.utils.data.DataLoader(train_split, batch_size=batch_size,\n",
        "                                               shuffle=True, drop_last=True)\n",
        "valid_dataloader = torch.utils.data.DataLoader(valid_split, batch_size=batch_size,\n",
        "                                               shuffle=True, drop_last=True)\n",
        "\n",
        "\n",
        "for i in range(len(model_layers)):\n",
        "    t1 = time.time()\n",
        "    \n",
        "    model_layer = model_layers[i]\n",
        "    #color = colors[i]\n",
        "\n",
        "    print(\"\\nTraining using {} layers:\".format(model_layer))\n",
        "    \n",
        "    h_unit_list = [int(h_units*1**(i+1)) for i in range(model_layer)]\n",
        "    #h_unit_list = [h_units for i in range(1,model_layer+1)]\n",
        "    print(h_unit_list)\n",
        "    model = ConvNet2(3, 10, None, h_unit_list).cuda()\n",
        "#   print(model)\n",
        "    summary(model, (3,64,64))\n",
        "    model.train()\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    optimizer = optim(model.parameters(), lr=lr)\n",
        "    loss_list, train_acc, valid_acc = train(train_dataloader, valid_dataloader,\n",
        "                                            iters=n_itr, model_save_path=\"model3.pth\")\n",
        "    _,_,test_acc = acc(test_dataloader)\n",
        "    t2 = time.time()\n",
        "    print(\"Time to converge: {} sec\".format(t2-t1))\n",
        "    print(\"Best train accuracy={}, valid accuracy={} (based on the later)\".format(train_acc, valid_acc))\n",
        "    print(\"Test accuracy on best model={}\".format(test_acc))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training using 5 layers:\n",
            "[64, 64, 64, 64, 64]\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 64, 64]           1,792\n",
            "       BatchNorm2d-2           [-1, 64, 64, 64]             128\n",
            "              ReLU-3           [-1, 64, 64, 64]               0\n",
            "            Conv2d-4           [-1, 64, 64, 64]          36,928\n",
            "       BatchNorm2d-5           [-1, 64, 64, 64]             128\n",
            "              ReLU-6           [-1, 64, 64, 64]               0\n",
            "         MaxPool2d-7           [-1, 64, 32, 32]               0\n",
            "            Conv2d-8           [-1, 64, 32, 32]          36,928\n",
            "       BatchNorm2d-9           [-1, 64, 32, 32]             128\n",
            "             ReLU-10           [-1, 64, 32, 32]               0\n",
            "           Conv2d-11           [-1, 64, 32, 32]          36,928\n",
            "      BatchNorm2d-12           [-1, 64, 32, 32]             128\n",
            "             ReLU-13           [-1, 64, 32, 32]               0\n",
            "        MaxPool2d-14           [-1, 64, 16, 16]               0\n",
            "           Conv2d-15           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-16           [-1, 64, 16, 16]             128\n",
            "             ReLU-17           [-1, 64, 16, 16]               0\n",
            "           Conv2d-18           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-19           [-1, 64, 16, 16]             128\n",
            "             ReLU-20           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-21             [-1, 64, 8, 8]               0\n",
            "           Conv2d-22             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-23             [-1, 64, 8, 8]             128\n",
            "             ReLU-24             [-1, 64, 8, 8]               0\n",
            "           Conv2d-25             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-26             [-1, 64, 8, 8]             128\n",
            "             ReLU-27             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-28             [-1, 64, 4, 4]               0\n",
            "           Conv2d-29             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-30             [-1, 64, 4, 4]             128\n",
            "             ReLU-31             [-1, 64, 4, 4]               0\n",
            "           Conv2d-32             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-33             [-1, 64, 4, 4]             128\n",
            "             ReLU-34             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-35             [-1, 64, 2, 2]               0\n",
            "          Dropout-36                   [-1, 64]               0\n",
            "           Linear-37                   [-1, 10]             650\n",
            "================================================================\n",
            "Total params: 336,074\n",
            "Trainable params: 336,074\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.05\n",
            "Forward/backward pass size (MB): 16.65\n",
            "Params size (MB): 1.28\n",
            "Estimated Total Size (MB): 17.98\n",
            "----------------------------------------------------------------\n",
            "Epoch 0: Loss=0.9612067798152566, Training Accuracy:0.5017528044871795, Validation Accuracy:0.4935897435897436\n",
            "Epoch 1: Loss=0.648292418802157, Training Accuracy:0.6920322516025641, Validation Accuracy:0.6686698717948718\n",
            "Epoch 2: Loss=0.5159408489707857, Training Accuracy:0.7056790865384616, Validation Accuracy:0.6769831730769231\n",
            "Epoch 3: Loss=0.4358959891833365, Training Accuracy:0.798652844551282, Validation Accuracy:0.7522035256410257\n",
            "Epoch 4: Loss=0.37356966361403465, Training Accuracy:0.7782201522435898, Validation Accuracy:0.7207532051282052\n",
            "Epoch 5: Loss=0.3238314522895962, Training Accuracy:0.8342848557692307, Validation Accuracy:0.7606169871794872\n",
            "Epoch 6: Loss=0.2839933359064162, Training Accuracy:0.8423477564102564, Validation Accuracy:0.7668269230769231\n",
            "Epoch 7: Loss=0.24956382275559008, Training Accuracy:0.869941907051282, Validation Accuracy:0.778145032051282\n",
            "Epoch 8: Loss=0.21970745018916205, Training Accuracy:0.8619290865384616, Validation Accuracy:0.7678285256410257\n",
            "Epoch 9: Loss=0.1925110489828512, Training Accuracy:0.902168469551282, Validation Accuracy:0.7889623397435898\n",
            "Epoch 10: Loss=0.16812473780009896, Training Accuracy:0.9110576923076923, Validation Accuracy:0.7938701923076923\n",
            "Epoch 11: Loss=0.14918051526183262, Training Accuracy:0.8997896634615384, Validation Accuracy:0.7776442307692307\n",
            "Epoch 12: Loss=0.12551280373008922, Training Accuracy:0.8863181089743589, Validation Accuracy:0.7639222756410257\n",
            "Epoch 13: Loss=0.10641508217668161, Training Accuracy:0.9487680288461539, Validation Accuracy:0.7955729166666666\n",
            "Epoch 14: Loss=0.09141142264707014, Training Accuracy:0.9145132211538461, Validation Accuracy:0.7756410256410257\n",
            "Epoch 15: Loss=0.08363746776012704, Training Accuracy:0.8231670673076923, Validation Accuracy:0.7054286858974359\n",
            "Epoch 16: Loss=0.07124149952142034, Training Accuracy:0.9332932692307693, Validation Accuracy:0.7881610576923077\n",
            "Epoch 17: Loss=0.0673733819858171, Training Accuracy:0.9562550080128205, Validation Accuracy:0.7962740384615384\n",
            "Epoch 18: Loss=0.05656381732842419, Training Accuracy:0.956229967948718, Validation Accuracy:0.7903645833333334\n",
            "Epoch 19: Loss=0.05290603227331303, Training Accuracy:0.9742337740384616, Validation Accuracy:0.8099959935897436\n",
            "Epoch 20: Loss=0.04679867494269274, Training Accuracy:0.9469901842948718, Validation Accuracy:0.7888621794871795\n",
            "Epoch 21: Loss=0.045528327987995, Training Accuracy:0.9610126201923077, Validation Accuracy:0.7941706730769231\n",
            "Epoch 22: Loss=0.04578547993878601, Training Accuracy:0.9520733173076923, Validation Accuracy:0.7875600961538461\n",
            "Epoch 23: Loss=0.0372015580360312, Training Accuracy:0.9795673076923077, Validation Accuracy:0.8083934294871795\n",
            "Epoch 24: Loss=0.03442007046396611, Training Accuracy:0.9347205528846154, Validation Accuracy:0.772636217948718\n",
            "Time to converge: 1683.4189677238464 sec\n",
            "Best train accuracy=0.9742337740384616, valid accuracy=0.8099959935897436 (based on the later)\n",
            "Test accuracy on best model=0.8117988782051282\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBflXygfgghL",
        "colab_type": "text"
      },
      "source": [
        "### Doubling filters after convolution \n",
        "\n",
        "Here we train with `16-32-64-128` filters in each layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNI8HPE5goUu",
        "colab_type": "code",
        "outputId": "1f48db17-426b-420b-e73e-c4a099a2b0e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1139
        }
      },
      "source": [
        "# torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "loss_type = \"Cross_Entropy\"\n",
        "model_layers = [5]\n",
        "h_units = 16\n",
        "optim = torch.optim.Adam\n",
        "lr = 0.0005\n",
        "\n",
        "train_split, valid_split = random_split(trainset, [ int(len(trainset)*0.8), \n",
        "                                        int(len(trainset)-(len(trainset)*0.8))])\n",
        "train_dataloader = torch.utils.data.DataLoader(train_split, batch_size=batch_size,\n",
        "                                               shuffle=True, drop_last=True)\n",
        "valid_dataloader = torch.utils.data.DataLoader(valid_split, batch_size=batch_size,\n",
        "                                               shuffle=True, drop_last=True)\n",
        "\n",
        "\n",
        "for i in range(len(model_layers)):\n",
        "    t1 = time.time()\n",
        "    \n",
        "    model_layer = model_layers[i]\n",
        "    #color = colors[i]\n",
        "\n",
        "    print(\"\\nTraining using {} layers:\".format(model_layer))\n",
        "    \n",
        "    h_unit_list = [int(h_units*2**(i)) for i in range(model_layer)]\n",
        "    #h_unit_list = [h_units for i in range(1,model_layer+1)]\n",
        "    print(h_unit_list)\n",
        "    model = ConvNet(3, 10, None, h_unit_list).cuda()\n",
        "#   print(model)\n",
        "    summary(model, (3,64,64))\n",
        "    model.train()\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    optimizer = optim(model.parameters(), lr=lr)\n",
        "    loss_list, train_acc, valid_acc = train(train_dataloader, valid_dataloader, \n",
        "                                            iters=n_itr, model_save_path=\"model4.pth\")\n",
        "    _, _, test_acc = acc(test_dataloader)\n",
        "    t2 = time.time()\n",
        "    print(\"Time to converge: {} sec\".format(t2-t1))\n",
        "    print(\"Best train accuracy={}, valid accuracy={} (based on the later)\".format(train_acc, valid_acc))\n",
        "    print(\"Test accuracy on best model={}\".format(test_acc))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training using 5 layers:\n",
            "[16, 32, 64, 128, 256]\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 64, 64]             448\n",
            "       BatchNorm2d-2           [-1, 16, 64, 64]              32\n",
            "              ReLU-3           [-1, 16, 64, 64]               0\n",
            "         MaxPool2d-4           [-1, 16, 32, 32]               0\n",
            "            Conv2d-5           [-1, 32, 32, 32]           4,640\n",
            "       BatchNorm2d-6           [-1, 32, 32, 32]              64\n",
            "              ReLU-7           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-8           [-1, 32, 16, 16]               0\n",
            "            Conv2d-9           [-1, 64, 16, 16]          18,496\n",
            "      BatchNorm2d-10           [-1, 64, 16, 16]             128\n",
            "             ReLU-11           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-12             [-1, 64, 8, 8]               0\n",
            "           Conv2d-13            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-14            [-1, 128, 8, 8]             256\n",
            "             ReLU-15            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-16            [-1, 128, 4, 4]               0\n",
            "           Conv2d-17            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-18            [-1, 256, 4, 4]             512\n",
            "             ReLU-19            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-20            [-1, 256, 2, 2]               0\n",
            "          Dropout-21                  [-1, 256]               0\n",
            "           Linear-22                   [-1, 10]           2,570\n",
            "================================================================\n",
            "Total params: 396,170\n",
            "Trainable params: 396,170\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.05\n",
            "Forward/backward pass size (MB): 3.15\n",
            "Params size (MB): 1.51\n",
            "Estimated Total Size (MB): 4.71\n",
            "----------------------------------------------------------------\n",
            "Epoch 0: Loss=0.9102157573215663, Training Accuracy:0.4772636217948718, Validation Accuracy:0.46304086538461536\n",
            "Epoch 1: Loss=0.678957594325766, Training Accuracy:0.6333133012820513, Validation Accuracy:0.6098758012820513\n",
            "Epoch 2: Loss=0.5739475777372718, Training Accuracy:0.6712239583333334, Validation Accuracy:0.6536458333333334\n",
            "Epoch 3: Loss=0.5078175037633628, Training Accuracy:0.7029747596153846, Validation Accuracy:0.6663661858974359\n",
            "Epoch 4: Loss=0.45762682775966823, Training Accuracy:0.7226312099358975, Validation Accuracy:0.6686698717948718\n",
            "Epoch 5: Loss=0.4100808771327138, Training Accuracy:0.7263371394230769, Validation Accuracy:0.6796875\n",
            "Epoch 6: Loss=0.3745653813239187, Training Accuracy:0.8011067708333334, Validation Accuracy:0.7289663461538461\n",
            "Epoch 7: Loss=0.3371937581105158, Training Accuracy:0.7312199519230769, Validation Accuracy:0.6625600961538461\n",
            "Epoch 8: Loss=0.3100205210503191, Training Accuracy:0.7305939503205128, Validation Accuracy:0.6464342948717948\n",
            "Epoch 9: Loss=0.28068659000564367, Training Accuracy:0.7991035657051282, Validation Accuracy:0.7007211538461539\n",
            "Epoch 10: Loss=0.26239190821070224, Training Accuracy:0.8200120192307693, Validation Accuracy:0.7137419871794872\n",
            "Epoch 11: Loss=0.2349059369880706, Training Accuracy:0.8517878605769231, Validation Accuracy:0.7340745192307693\n",
            "Epoch 12: Loss=0.21163656178396195, Training Accuracy:0.8886217948717948, Validation Accuracy:0.7418870192307693\n",
            "Epoch 13: Loss=0.18746873590862378, Training Accuracy:0.8718699919871795, Validation Accuracy:0.7271634615384616\n",
            "Epoch 14: Loss=0.16856007621390745, Training Accuracy:0.8512620192307693, Validation Accuracy:0.7162459935897436\n",
            "Epoch 15: Loss=0.1526216752245091, Training Accuracy:0.8621043669871795, Validation Accuracy:0.7151442307692307\n",
            "Epoch 16: Loss=0.14076263114111498, Training Accuracy:0.9152894631410257, Validation Accuracy:0.7428886217948718\n",
            "Epoch 17: Loss=0.12457769794855267, Training Accuracy:0.9032451923076923, Validation Accuracy:0.7329727564102564\n",
            "Epoch 18: Loss=0.1122251451597549, Training Accuracy:0.9254306891025641, Validation Accuracy:0.7420873397435898\n",
            "Epoch 19: Loss=0.10164521398837678, Training Accuracy:0.8700170272435898, Validation Accuracy:0.6849959935897436\n",
            "Epoch 20: Loss=0.08791201701387763, Training Accuracy:0.9432091346153846, Validation Accuracy:0.7412860576923077\n",
            "Epoch 21: Loss=0.08132219378603622, Training Accuracy:0.9563802083333334, Validation Accuracy:0.7540064102564102\n",
            "Epoch 22: Loss=0.07764154634787701, Training Accuracy:0.9253305288461539, Validation Accuracy:0.7261618589743589\n",
            "Epoch 23: Loss=0.07327586854808033, Training Accuracy:0.9465645032051282, Validation Accuracy:0.7402844551282052\n",
            "Epoch 24: Loss=0.06814033632690553, Training Accuracy:0.9333433493589743, Validation Accuracy:0.7224559294871795\n",
            "Time to converge: 639.618234872818 sec\n",
            "Best train accuracy=0.9563802083333334, valid accuracy=0.7540064102564102 (based on the later)\n",
            "Test accuracy on best model=0.7408854166666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "13M6ruQVOkIS"
      },
      "source": [
        "## Average Pooling on Best Model of Max Pooling (Overall Best Model)\n",
        "------\n",
        "Average Pooling is used to replace all the subsampling layers in our best model where the accuracy is `0.8118`. The average pooling version gives an accuracy of `0.8220`. Therefore in our case, average pooling seems to work better than max pooling and forms the best model we have."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wa-LNooNOk5G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1394
        },
        "outputId": "a89468de-550f-406b-b633-fc24cfed5ebb"
      },
      "source": [
        "model_layers = [5]\n",
        "h_units = 64\n",
        "optim = torch.optim.Adam\n",
        "lr = 0.0005\n",
        "\n",
        "# print(axes)\n",
        "\n",
        "train_split, valid_split = random_split(trainset, [ int(len(trainset)*0.8), \n",
        "                                        int(len(trainset)-(len(trainset)*0.8))])\n",
        "train_dataloader = torch.utils.data.DataLoader(train_split, batch_size=batch_size,\n",
        "                                               shuffle=True, drop_last=True)\n",
        "valid_dataloader = torch.utils.data.DataLoader(valid_split, batch_size=batch_size,\n",
        "                                               shuffle=True, drop_last=True)\n",
        "\n",
        "\n",
        "for i in range(len(model_layers)):\n",
        "    t1 = time.time()\n",
        "    \n",
        "    model_layer = model_layers[i]\n",
        "    #color = colors[i]\n",
        "\n",
        "    print(\"\\nTraining using {} layers:\".format(model_layer))\n",
        "    \n",
        "    h_unit_list = [int(h_units*1**(i+1)) for i in range(model_layer)]\n",
        "    #h_unit_list = [h_units for i in range(1,model_layer+1)]\n",
        "    print(h_unit_list)\n",
        "    model = ConvNet2(3, 10, None, h_unit_list, pool='mean').cuda()\n",
        "#   print(model)\n",
        "    summary(model, (3,64,64))\n",
        "    model.train()\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    optimizer = optim(model.parameters(), lr=lr)\n",
        "    loss_list, train_acc, valid_acc = train(train_dataloader, valid_dataloader,\n",
        "                                            iters=n_itr, model_save_path=\"model5.pth\")\n",
        "    _,_,test_acc = acc(test_dataloader)\n",
        "    t2 = time.time()\n",
        "    print(\"Time to converge: {} sec\".format(t2-t1))\n",
        "    print(\"Best train accuracy={}, valid accuracy={} (based on the later)\".format(train_acc, valid_acc))\n",
        "    print(\"Test accuracy on best model={}\".format(test_acc))\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training using 5 layers:\n",
            "[64, 64, 64, 64, 64]\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 64, 64]           1,792\n",
            "       BatchNorm2d-2           [-1, 64, 64, 64]             128\n",
            "              ReLU-3           [-1, 64, 64, 64]               0\n",
            "            Conv2d-4           [-1, 64, 64, 64]          36,928\n",
            "       BatchNorm2d-5           [-1, 64, 64, 64]             128\n",
            "              ReLU-6           [-1, 64, 64, 64]               0\n",
            "         MaxPool2d-7           [-1, 64, 32, 32]               0\n",
            "            Conv2d-8           [-1, 64, 32, 32]          36,928\n",
            "       BatchNorm2d-9           [-1, 64, 32, 32]             128\n",
            "             ReLU-10           [-1, 64, 32, 32]               0\n",
            "           Conv2d-11           [-1, 64, 32, 32]          36,928\n",
            "      BatchNorm2d-12           [-1, 64, 32, 32]             128\n",
            "             ReLU-13           [-1, 64, 32, 32]               0\n",
            "        AvgPool2d-14           [-1, 64, 16, 16]               0\n",
            "           Conv2d-15           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-16           [-1, 64, 16, 16]             128\n",
            "             ReLU-17           [-1, 64, 16, 16]               0\n",
            "           Conv2d-18           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-19           [-1, 64, 16, 16]             128\n",
            "             ReLU-20           [-1, 64, 16, 16]               0\n",
            "        AvgPool2d-21             [-1, 64, 8, 8]               0\n",
            "           Conv2d-22             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-23             [-1, 64, 8, 8]             128\n",
            "             ReLU-24             [-1, 64, 8, 8]               0\n",
            "           Conv2d-25             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-26             [-1, 64, 8, 8]             128\n",
            "             ReLU-27             [-1, 64, 8, 8]               0\n",
            "        AvgPool2d-28             [-1, 64, 4, 4]               0\n",
            "           Conv2d-29             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-30             [-1, 64, 4, 4]             128\n",
            "             ReLU-31             [-1, 64, 4, 4]               0\n",
            "           Conv2d-32             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-33             [-1, 64, 4, 4]             128\n",
            "             ReLU-34             [-1, 64, 4, 4]               0\n",
            "        AvgPool2d-35             [-1, 64, 2, 2]               0\n",
            "          Dropout-36                   [-1, 64]               0\n",
            "           Linear-37                   [-1, 10]             650\n",
            "================================================================\n",
            "Total params: 336,074\n",
            "Trainable params: 336,074\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.05\n",
            "Forward/backward pass size (MB): 16.65\n",
            "Params size (MB): 1.28\n",
            "Estimated Total Size (MB): 17.98\n",
            "----------------------------------------------------------------\n",
            "Epoch 0: Loss=1.06687050499022, Training Accuracy:0.48029346955128205, Validation Accuracy:0.47055288461538464\n",
            "Epoch 1: Loss=0.7299996439833194, Training Accuracy:0.6399489182692307, Validation Accuracy:0.6238982371794872\n",
            "Epoch 2: Loss=0.5677236968185753, Training Accuracy:0.6331380208333334, Validation Accuracy:0.6144831730769231\n",
            "Epoch 3: Loss=0.46970558003522456, Training Accuracy:0.7628956330128205, Validation Accuracy:0.7329727564102564\n",
            "Epoch 4: Loss=0.3999505710089579, Training Accuracy:0.7367788461538461, Validation Accuracy:0.7077323717948718\n",
            "Epoch 5: Loss=0.3523704226827249, Training Accuracy:0.8062900641025641, Validation Accuracy:0.7633213141025641\n",
            "Epoch 6: Loss=0.31162389426026493, Training Accuracy:0.814453125, Validation Accuracy:0.7619190705128205\n",
            "Epoch 7: Loss=0.2833481641719118, Training Accuracy:0.8169571314102564, Validation Accuracy:0.7682291666666666\n",
            "Epoch 8: Loss=0.2571089850971475, Training Accuracy:0.7664012419871795, Validation Accuracy:0.7215544871794872\n",
            "Epoch 9: Loss=0.23324842110741884, Training Accuracy:0.8421223958333334, Validation Accuracy:0.7842548076923077\n",
            "Epoch 10: Loss=0.21291304973419756, Training Accuracy:0.8726712740384616, Validation Accuracy:0.8022836538461539\n",
            "Epoch 11: Loss=0.1938833869062364, Training Accuracy:0.881660657051282, Validation Accuracy:0.8049879807692307\n",
            "Epoch 12: Loss=0.18056125135626644, Training Accuracy:0.8780048076923077, Validation Accuracy:0.7978766025641025\n",
            "Epoch 13: Loss=0.16128656303044409, Training Accuracy:0.8803335336538461, Validation Accuracy:0.7920673076923077\n",
            "Epoch 14: Loss=0.14341530407546088, Training Accuracy:0.9034455128205128, Validation Accuracy:0.8105969551282052\n",
            "Epoch 15: Loss=0.13319461452192627, Training Accuracy:0.9011668669871795, Validation Accuracy:0.8064903846153846\n",
            "Epoch 16: Loss=0.1207403285370674, Training Accuracy:0.9204727564102564, Validation Accuracy:0.8204126602564102\n",
            "Epoch 17: Loss=0.11010953187360428, Training Accuracy:0.8983122996794872, Validation Accuracy:0.797676282051282\n",
            "Epoch 18: Loss=0.09895072344806977, Training Accuracy:0.9125100160256411, Validation Accuracy:0.8020833333333334\n",
            "Epoch 19: Loss=0.09338614813168533, Training Accuracy:0.90625, Validation Accuracy:0.7944711538461539\n",
            "Epoch 20: Loss=0.08565967419417575, Training Accuracy:0.9261818910256411, Validation Accuracy:0.8098958333333334\n",
            "Epoch 21: Loss=0.07520497070800047, Training Accuracy:0.8890224358974359, Validation Accuracy:0.7858573717948718\n",
            "Epoch 22: Loss=0.06713411190139595, Training Accuracy:0.9534505208333334, Validation Accuracy:0.8238181089743589\n",
            "Epoch 23: Loss=0.06400667339039501, Training Accuracy:0.8866185897435898, Validation Accuracy:0.7816506410256411\n",
            "Epoch 24: Loss=0.06023797340458259, Training Accuracy:0.9407552083333334, Validation Accuracy:0.8075921474358975\n",
            "Time to converge: 1688.284479379654 sec\n",
            "Best train accuracy=0.9534505208333334, valid accuracy=0.8238181089743589 (based on the later)\n",
            "Test accuracy on best model=0.8220152243589743\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}